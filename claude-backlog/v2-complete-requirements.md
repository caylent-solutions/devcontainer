# Caylent DevContainer CLI v2.0.0 — Complete Requirements Specification

## Document Structure

This document contains three work units to be implemented in order:

1. **CLI v2.0.0 Core Requirements** — Feature and change list for the CLI rewrite
2. **Remote DevContainer Catalog Support** — Unified catalog architecture for devcontainer configurations
3. **Mirror DevContainer Base Image to ECR Public** — ECR mirror for global low-latency image pulls

All code must follow the coding standards defined in `CLAUDE.md`. All changes must be deeply tested with unit and functional tests.

---
---

# Work Unit 1: CLI v2.0.0 Core Requirements

## Rules

- Do not repeat or duplicate any logic
- Reuse and upgrade existing logic with new features
- All shared behavior must be implemented once and consumed by all callers
- This is a major release (v2.0.0) — there is no backward compatibility with v1.x
  - All v1.x templates must be recreated by the user
  - The old `env_values` template format is not supported — only `containerEnv`
  - Templates without v2.0.0 metadata are rejected with a clear error
  - If a user attempts to use a v1.x template, exit non-zero with an error explaining they must recreate it using `cdevcontainer template create`
- Python minimum version is `>=3.10` — update `python_requires` in `pyproject.toml`
- CLI is installed exclusively via `pipx` — no `install`/`uninstall` commands, no `bin/cdevcontainer` entry point script
- No `-y`/`--yes` flag — all prompts require explicit user interaction

---

## Breaking Changes Summary (v1.x → v2.0.0)

- `containerEnv` block removed from `devcontainer.json` — environment variables are sourced from `shell.env` by the `postCreateCommand`
- `CICD` removed from templates, `EXAMPLE_ENV_VALUES`, and project files — controlled purely by runtime environment with postcreate defaulting to `"false"`
- `env_values` template format no longer supported — only `containerEnv`
- `GIT_AUTH_METHOD` is now a required key in all templates
- `GIT_PROVIDER_URL` must be a hostname only — no protocol prefix
- `HOST_PROXY` and `HOST_PROXY_URL` added as built-in keys
- All project files and templates require metadata (`template_name`, `template_path`, `cli_version`)
- `.devcontainer/catalog-entry.json` persists in the project repo — tracks which catalog and collection the project's devcontainer configuration came from
- `.tool-versions` created as empty file instead of containing Python version
- `code` command no longer sources `shell.env` before launching IDE
- `code` command no longer generates `shell.env` — it is generated by `setup-devcontainer` and `template load`
- `-y`/`--yes` flag removed entirely — all prompts require explicit user interaction
- `env` subcommand removed (`env export`, `env load`) — obsolete since `shell.env` is generated alongside JSON
- `install` and `uninstall` commands removed — CLI is installed exclusively via `pipx`
- `bin/cdevcontainer` entry point script removed — `pipx` creates the executable from `pyproject.toml` entry point
- Python minimum version raised to `>=3.10` (Python 3.8 and 3.9 are EOL)
- `NO_PROXY`/`no_proxy` simplified to `localhost,127.0.0.1,.local` — Ubuntu and Microsoft domains removed so `apt-get` can use proxy when `HOST_PROXY=true`
- `postCreateCommand` uses `sudo -E` to preserve proxy environment variables through `sudo`
- `code` command gains `--regenerate-shell-env` flag to regenerate `shell.env` from existing JSON
- `setup-devcontainer` uses unified catalog pipeline — this repo refactored to catalog structure (see Work Unit 2: Remote DevContainer Catalog Support)
- New `catalog` subcommand with `list` and `validate` options
- `DEVCONTAINER_CATALOG_URL` ENV var support for specialized catalog repos

---

## DRY Refactoring Requirements

The following consolidations must be implemented as part of v2.0.0. All shared behavior must exist as a single implementation consumed by every caller.

### A. JSON File Operations

- **`write_json_file(path, data)`** — Single utility in `utils/fs.py` for writing JSON with `indent=2` and trailing newline. Replaces all 9 current inline `json.dump` + `f.write("\n")` patterns.
- **`load_json_config(path)`** — Already exists in `utils/fs.py`. All JSON reading must use this function. Remove all inline `open() + json.load()` patterns.

### B. Project File Generation

- **`write_project_files(project_root, template_data, template_name, template_path)`** — Single function in `utils/fs.py` that:
  1. Writes `devcontainer-environment-variables.json` with metadata and sorted keys
  2. Immediately generates `shell.env` with metadata comment header and sorted exports
  3. Writes `.devcontainer/aws-profile-map.json` if AWS is enabled
  4. Writes `.devcontainer/ssh-private-key` if `GIT_AUTH_METHOD=ssh`
  5. Ensures `.gitignore` entries for all sensitive files
- Every command that produces project files must call this one function: `setup-devcontainer`, `template load`, and any validation flow that regenerates files.

### C. Template Application

- **Merge `apply_template()` and `apply_template_without_clone()`** into one function: `apply_template(template_data, target_path)`. This function handles only template data processing and environment file generation via `write_project_files()`. It does NOT copy `.devcontainer/` files — that responsibility belongs to the catalog pipeline's `copy_collection_to_project()` (see Work Unit 2).
- **Merge `interactive_setup()` and `interactive_setup_without_clone()`** into one function: `interactive_setup(target_path)`. Handles the interactive template selection/creation flow and calls `apply_template()` for file generation.

### D. Template Utilities

- **`get_template_path(name) -> str`** — Single function for `os.path.join(TEMPLATES_DIR, f"{name}.json")`. Replaces 7 inline constructions.
- **`get_template_names() -> list[str]`** — Single function that scans `TEMPLATES_DIR` and returns template names. `template list` calls it for display. `setup_interactive` calls it for selection choices.
- **`ensure_templates_dir()`** — Single function in a shared utility. Replaces 3 implementations.
- **`validate_template(template_data) -> template_data`** — Single validation function called on every template load. See "Template Validation on Load" section.

### E. Missing Variable Detection

- **`get_missing_env_vars(container_env: dict) -> dict`** — Single function that checks against `EXAMPLE_ENV_VALUES`. Replaces `code.py:check_missing_env_vars()` and `template.py:get_missing_single_line_vars()`.

### F. Version Compatibility

- **`check_template_version(template_data)`** — Single function that validates `cli_version` matches current CLI major version. For v2.0.0, rejects any template without `cli_version` or with a v1.x version. Replaces 3 separate semver comparison implementations.

### G. Confirmation and Prompt Utilities

- **Remove `setup.py:confirm_overwrite()`** — All confirmation prompts use `ui.py:confirm_action()`.
- **Remove `AUTO_YES` / `set_auto_yes()`** — The `-y`/`--yes` flag is removed entirely in v2.0.0. All prompts require explicit user interaction. Remove all `AUTO_YES` checks and the `set_auto_yes()` function from `utils/ui.py`.
- **`ask_or_exit(questionary_question)`** — Wrapper that calls `.ask()`, checks for `None`, and calls `exit_cancelled()`. Replaces 14 inline null-check patterns in `setup_interactive.py`.
- **`exit_cancelled(message="Operation cancelled by user")`** — Single function in `utils/ui.py`. Replaces ~20 scattered cancellation exit patterns.
- **`exit_with_error(message)`** — Single function in `utils/ui.py` that logs the error and calls `sys.exit(1)`. Replaces ~40 inline `import sys; sys.exit(1)` patterns.

### H. File Path Constants

Add to `utils/constants.py`:

- `ENV_VARS_FILENAME = "devcontainer-environment-variables.json"`
- `SHELL_ENV_FILENAME = "shell.env"`
- `EXAMPLE_ENV_FILE = "example-container-env-values.json"`
- `EXAMPLE_AWS_FILE = "example-aws-profile-map.json"`
- `CATALOG_ENTRY_FILENAME = "catalog-entry.json"`
- `SSH_KEY_FILENAME = "ssh-private-key"`

Replace all hardcoded filename strings throughout the codebase.

### I. CLI_NAME Constant

Remove `CLI_NAME` from `cli.py`. Import from `utils/constants.py` where it already exists.

### J. Removed Commands

- **Remove `env` subcommand** — `env export` and `env load` are obsolete. `shell.env` is now generated alongside `devcontainer-environment-variables.json` by `write_project_files()`. Remove `commands/env.py` entirely.
- **Remove `install` and `uninstall` commands** — CLI is installed exclusively via `pipx install caylent-devcontainer-cli`. Remove `commands/install.py` entirely.
- **Remove `bin/cdevcontainer`** — The shell entry point script is redundant. `pyproject.toml` already defines the entry point `cdevcontainer = "caylent_devcontainer_cli.cli:main"` which `pipx` uses to create the executable.
- **Remove `-y`/`--yes` argument** from all command parsers and the `main()` argument parser. Remove 12 identical `add_argument` calls.

### K. Project Root Resolution

- **`resolve_project_root(path=None) -> str`** — Single function that defaults to `os.getcwd()` and validates `.devcontainer/` exists. Replaces `find_project_root()` and the 3 inline `args.project_root or os.getcwd()` patterns.

### L. Example File Removal

- **`remove_example_files(target_devcontainer)`** — Single function that removes example JSON files from `.devcontainer/`. Replaces 2 duplicated implementations.

### M. Import Hygiene

- Import `sys` at module level in every file that uses it. Remove all inline `import sys` inside function bodies.
- Import `COLORS` at module level where used. Remove inline imports.

---

## Universal UI Requirements

### Input Confirmation

Every command that accepts user input must give the user the opportunity to review and re-enter their input.

After each input is received:

1. Display the entered value back to the user:
   - **Single-line text:** Show the value as entered
   - **Password fields (e.g., `GIT_TOKEN`):** Show masked value (e.g., `****...****`) or length
   - **Select fields:** Show the selected option
   - **Multi-line text (AWS profiles):** Show the parsed/formatted result, not the raw input
   - **File path inputs (SSH key):** Show the key fingerprint from `ssh-keygen -l -f <keyfile>`
2. Ask: "Is this correct?" (yes/no)
3. If no: re-prompt for the same input
4. If yes: proceed to the next prompt

### Questionary Null Handling

All `questionary` prompts must use the shared `ask_or_exit()` wrapper. If the user cancels (Ctrl+C or null return), exit cleanly with `exit_cancelled()`.

---

## Environment Variable Reference

### EXAMPLE_ENV_VALUES (Base Keys)

These keys must exist in every template and every project's `devcontainer-environment-variables.json`. The values shown are examples used as defaults during template creation — they are never used in real environments.

```python
EXAMPLE_ENV_VALUES = {
    "AWS_CONFIG_ENABLED": "true",
    "AWS_DEFAULT_OUTPUT": "json",
    "DEFAULT_GIT_BRANCH": "main",
    "DEFAULT_PYTHON_VERSION": "3.12.9",
    "DEVELOPER_NAME": "Your Name",
    "EXTRA_APT_PACKAGES": "",
    "GIT_AUTH_METHOD": "token",
    "GIT_PROVIDER_URL": "github.com",
    "GIT_TOKEN": "your-git-token",
    "GIT_USER": "your-username",
    "GIT_USER_EMAIL": "your-email@example.com",
    "HOST_PROXY": "false",
    "HOST_PROXY_URL": "",
    "PAGER": "cat",
}
```

**Removed from v1.x:** `CICD` — no longer a template/project file variable. Controlled by runtime environment. Postcreate script defaults to `"false"` via `${CICD:-false}`.

**Conditional behavior:**
- `GIT_TOKEN` — required only when `GIT_AUTH_METHOD=token`. Must not exist when `GIT_AUTH_METHOD=ssh`.
- `HOST_PROXY_URL` — always present in templates and project files. Set to empty string `""` when `HOST_PROXY=false`. Its value is only validated (must start with `http://` or `https://`) when `HOST_PROXY=true`.

### Known Built-In Keys

The `KNOWN_KEYS` set includes all `EXAMPLE_ENV_VALUES` keys. This set is used for conflict detection during free-form custom variable entry and template validation.

### Static Container Values

The following values are NOT stored in templates or `EXAMPLE_ENV_VALUES`. They are generated directly into `shell.env` by the `write_project_files()` function:

- `DEVCONTAINER=true`
- `BASH_ENV=${containerWorkspaceFolder}/shell.env`
- `NO_PROXY=localhost,127.0.0.1,.local`
- `no_proxy=localhost,127.0.0.1,.local`
- Dynamic `PATH` with asdf shims and `.localscripts`
- `unset GIT_EDITOR`

When `HOST_PROXY=true`, also generate:
- `HTTP_PROXY=${HOST_PROXY_URL}`
- `HTTPS_PROXY=${HOST_PROXY_URL}`
- `http_proxy=${HOST_PROXY_URL}`
- `https_proxy=${HOST_PROXY_URL}`

---

## Input Constraints

### Constrained Select Fields (must be one of the listed values)

| Variable | Choices | Default |
|---|---|---|
| `AWS_CONFIG_ENABLED` | `true`, `false` | `true` |
| `AWS_DEFAULT_OUTPUT` | `json`, `table`, `text`, `yaml` | `json` |
| `GIT_AUTH_METHOD` | `token`, `ssh` | `token` |
| `HOST_PROXY` | `true`, `false` | `false` |
| `PAGER` | `cat`, `less`, `more`, `most` | `cat` |

### Constrained Text Fields

| Variable | Constraint | Error Message |
|---|---|---|
| `HOST_PROXY_URL` | Must start with `http://` or `https://` | "Host proxy URL must start with http:// or https://" |
| `GIT_PROVIDER_URL` | Must be hostname only — no `http://`, `https://`, `ssh://`, or `git@` prefix. Must contain at least one dot. | "Git provider URL must be a hostname only (e.g., github.com). Do not include a protocol prefix." |

### Text Fields (non-empty only)

| Variable | Default |
|---|---|
| `DEFAULT_GIT_BRANCH` | `main` |
| `DEFAULT_PYTHON_VERSION` | `3.12.9` |
| `DEVELOPER_NAME` | none |
| `GIT_USER` | none |
| `GIT_USER_EMAIL` | none |
| `GIT_TOKEN` | none (password field, only if `GIT_AUTH_METHOD=token`) |

### Text Fields (may be empty)

| Variable | Default |
|---|---|
| `EXTRA_APT_PACKAGES` | `""` |

### SSH Key Input (only if `GIT_AUTH_METHOD=ssh`)

- Prompt: "Enter the path to your SSH private key file" (e.g., `~/.ssh/id_ed25519`)
- Validation steps after reading:
  1. File exists and is readable — if not, show error and re-prompt
  2. Normalize line endings — strip `\r`, ensure trailing newline
  3. Format check — content starts with `-----BEGIN` and contains `-----END`
  4. Real key validation — run `ssh-keygen -y -f <keyfile>` non-interactively. If this fails:
     - If passphrase required: "This key requires a passphrase. Passphrase-protected keys are not supported. Please provide a passphrase-free key or remove the passphrase with `ssh-keygen -p -f <keyfile>`"
     - If invalid/corrupt: show the error output from `ssh-keygen`
     - Re-prompt in both cases
- On success: display key fingerprint from `ssh-keygen -l -f <keyfile>`, ask "Is this correct?"

### Free-Form Custom Variable Keys

- Must be non-empty
- Must not conflict with any key in `KNOWN_KEYS` (all `EXAMPLE_ENV_VALUES` keys)
- Must not conflict with any key already added by a built-in prompt in the current session (e.g., `HOST_PROXY`, `HOST_PROXY_URL`)
- Must not conflict with any custom key already entered earlier in the same free-form loop
- On conflict: "The key '<key>' already exists (<source>). Please enter a different key name." — then re-prompt

### Free-Form Custom Variable Values

- No constraints — may be any string including empty

---

## Template Validation on Load

Every command that reads a template must call the shared `validate_template()` function before using the data.

### Structural Validation

- `containerEnv` must exist and be a dict — exit non-zero if not
- `cli_version` must exist and be a v2.x version — if missing or v1.x, exit non-zero:
  - "This template was created with CLI v1.x and is not compatible with v2.0.0. Please recreate your template using `cdevcontainer template create <name>`"
- `template_name` must exist — exit non-zero if not
- `template_path` must exist — exit non-zero if not

### Base Key Completeness

- Verify all `EXAMPLE_ENV_VALUES` keys exist in `containerEnv` (respecting conditional requirements — `GIT_TOKEN` only when `GIT_AUTH_METHOD=token`; `HOST_PROXY_URL` is always present but its value is only validated when `HOST_PROXY=true`)
- If any are missing, present each missing key with its default value and prompt the user to accept the default or enter a custom value

### Known Key Value Validation

- `AWS_CONFIG_ENABLED` must be `"true"` or `"false"`
- `HOST_PROXY` must be `"true"` or `"false"`
- `GIT_AUTH_METHOD` must be `"token"` or `"ssh"`
- `HOST_PROXY_URL` must start with `http://` or `https://` (only validated when `HOST_PROXY=true`)
- `GIT_PROVIDER_URL` must be hostname only — no protocol prefix, must contain at least one dot
- If any value is invalid, show the current value, explain what is expected, and prompt for correction

### Auth Method Consistency

- If `GIT_AUTH_METHOD=token`: `GIT_TOKEN` must exist and be non-empty. `ssh_private_key` must NOT exist.
- If `GIT_AUTH_METHOD=ssh`: `GIT_TOKEN` must NOT exist. `ssh_private_key` must exist.
- If both `GIT_TOKEN` and `ssh_private_key` are present: prompt user to choose one, remove the other

### Conflict Detection

- If the CLI version has added new known keys that already exist in the template as user-added custom keys (detectable by comparing template's `cli_version` to current version), flag each one:
  - Show the key name, the value currently in the template, and the CLI's expected default
  - Prompt: keep the existing value, use the CLI default, or enter a new value

### Where This Runs

One shared `validate_template()` function called by: `template load`, `template upgrade`, `setup-devcontainer` (interactive), and `code` (for template comparison).

---

## File Generation

### When Files Are Generated

`devcontainer-environment-variables.json` and `shell.env` are always generated together, immediately, by any command that produces or updates project configuration:

- `setup-devcontainer` — after interactive or manual setup completes
- `template load` — after loading template into project
- Any validation flow in `code` or `setup-devcontainer` that regenerates files

All file generation goes through the single `write_project_files()` function.

### Metadata Requirements

#### `shell.env`

Add a comment block at the top of the file:

```bash
# Template: <template_name>
# Template Path: <template_path>
# CLI Version: <cli_version>
# Generated: <ISO 8601 timestamp>
```

#### `devcontainer-environment-variables.json`

Add flat top-level metadata keys alongside `containerEnv`:

```json
{
  "template_name": "<template_name>",
  "template_path": "<template_path>",
  "cli_version": "<cli_version>",
  "containerEnv": {
    ...
  }
}
```

### Sorting Requirements

- In `shell.env`: all export lines sorted in **ascending alphabetical order** (A→Z) by variable name
- In `devcontainer-environment-variables.json`: all keys under `containerEnv` sorted in **ascending alphabetical order** (A→Z)

### Sensitive File Protection

The following files must be in `.gitignore`. The `write_project_files()` function ensures these entries exist:

- `shell.env`
- `devcontainer-environment-variables.json`
- `.devcontainer/aws-profile-map.json`
- `.devcontainer/ssh-private-key`

---

## ARG: setup-devcontainer

### Current Behavior Being Changed

Currently `setup-devcontainer`:

1. Checks if `.devcontainer/` directory exists
2. If exists: reads `VERSION` file, asks to overwrite
3. If overwriting: clones repo, copies `.devcontainer/`, runs interactive setup or manual mode which creates `devcontainer-environment-variables.json`
4. If not overwriting: skips clone, still runs interactive setup which creates `devcontainer-environment-variables.json`
5. Creates `.tool-versions` with Python version if missing
6. Updates `.gitignore`
7. Does NOT generate `shell.env`

**v2.0.0 change:** The "clone repo, copy `.devcontainer/`" step is replaced by the unified catalog pipeline. Instead of directly copying `.devcontainer/` from a cloned repo, `setup-devcontainer` uses `clone_catalog_repo()` → `discover_collections()` → `copy_collection_to_project()`. See Work Unit 2: Remote DevContainer Catalog Support for the full catalog architecture. All references to "clone and copy" below refer to this catalog-based flow.

### New Behavior

#### .tool-versions

- Create `.tool-versions` as an **empty file** if it does not already exist
- Do not add any runtime entries to it

#### Existing Configuration Detection

When `setup-devcontainer` detects that `.devcontainer/` already exists:

- Inform the user that devcontainer configuration files already exist
- Show the current version from `VERSION` file (if it exists; if not, display "version unknown")
- If `.devcontainer/catalog-entry.json` exists, show the collection name and catalog URL from its metadata so the user knows where their current configuration came from
- Inform the user they will be asked whether to replace the configuration

**Python notice:** If `.tool-versions` already exists **and** contains a Python entry:

- Append to the replacement prompt:
  - The recommended configuration manages Python through `"features": {}` in `devcontainer.json`, `.devcontainer/.devcontainer.postcreate.sh`, and `.devcontainer/devcontainer-functions.sh` as done in the default Caylent DevContainer CLI template
  - If they want to follow this recommendation, they should choose **yes** when prompted to replace

#### User Decision: Replace

If the user chooses to replace:

- Display a notification explaining:
  - Their existing devcontainer files will be overwritten
  - They should review the resulting code changes in git before building or rebuilding the devcontainer
  - If the project immediately starts in the devcontainer, they should:
    - Close the remote connection to the devcontainer
    - Open the project folder directly from the operating system file explorer
    - Avoid opening from "recently opened folders" to prevent auto-launching of the devcontainer
  - They will need to merge back any project-specific customizations while keeping new upgraded features
  - They must be careful not to introduce conflicts or errors
  - They should fully test these changes before pushing and merging them remotely
  - After completing all of this, they should rebuild the devcontainer
  - In some cases it may help to rebuild without cache depending on what has changed

- Require explicit acknowledgement:
  - Prompt the user to press any key to continue
  - Do not continue until a keypress is received

- Proceed with catalog selection and setup

#### User Decision: Do Not Replace

- Leave all existing `.devcontainer/` files unchanged
- Continue with interactive setup for environment files only (using the unified `apply_template()` — no `.devcontainer/` file copying since the user chose not to replace)

#### File Validation (Both Paths) — Informational Only

After the replace/no-replace decision, if `devcontainer-environment-variables.json` and `shell.env` already exist, run the shared validation detection logic (Steps 0–3 from "Behavior When Files Already Exist" below) to identify any issues.

Unlike the `code` command, `setup-devcontainer` does **not** prompt the user to fix each issue (Steps 4–5). Instead, it displays the issues as **informational** and continues:

- List any missing or invalid variables detected
- Display: "The following configuration issues were detected in your current project files. Completing this setup will regenerate your project configuration from the selected template and resolve these issues."
- Continue directly into the interactive setup flow

This avoids redundant prompting — the interactive setup flow that follows inherently resolves all detected issues by regenerating both files via `write_project_files()`.

#### File Generation

After interactive setup completes (template selected or created), `write_project_files()` generates both `devcontainer-environment-variables.json` and `shell.env` together.

---

## ARG: code

### Current Behavior Being Changed

Currently `code`:

1. Finds project root with `.devcontainer/`
2. Requires `devcontainer-environment-variables.json` to exist — errors if missing
3. Checks for missing env vars against hardcoded `EXAMPLE_ENV_VALUES`
4. Generates `shell.env` if missing or stale
5. Ensures `.gitignore` entries
6. Verifies IDE command is in PATH
7. Runs `source shell.env && code <project_root>` — sources env vars into the IDE process

### New Behavior

#### Environment Variable Handling

- The `code` command does **not** source `shell.env` before launching the IDE
- Environment variables are sourced by the `postCreateCommand` inside the devcontainer
- The launch command is simply: `<ide_command> <project_root>`

#### File Generation

- The `code` command does **not** generate `shell.env` by default
- Both `devcontainer-environment-variables.json` and `shell.env` must already exist — they are generated by `setup-devcontainer` or `template load`
- If either file is missing, exit non-zero with a clear error explaining which file is missing and how to generate it (e.g., "Run `cdevcontainer setup-devcontainer <path>` or `cdevcontainer template load <name> -p <path>` to generate project files")

#### `--regenerate-shell-env` Flag

When `cdevcontainer code --regenerate-shell-env <path>` is invoked:

1. Require `devcontainer-environment-variables.json` to exist — exit non-zero if missing
2. Read the existing JSON file (do NOT modify it)
3. Regenerate `shell.env` from the JSON data using the shell.env generation portion of `write_project_files()` — this produces a `shell.env` that matches the current JSON contents with metadata comment header, sorted exports, static container values, and proxy variables if `HOST_PROXY=true`
4. Continue with the normal validation steps and IDE launch

This flag is for the scenario where a user manually edits `devcontainer-environment-variables.json` (adding a variable, changing a value) and needs `shell.env` to reflect those changes without going through the full `setup-devcontainer` or `template load` workflow.

#### Behavior When Files Already Exist

When `code` is executed and both `devcontainer-environment-variables.json` and `shell.env` exist, perform the following validation steps before launching the IDE.

Both `code` and `setup-devcontainer` share the same validation detection logic (Steps 0–3) via a single shared function. They diverge on the response: `code` prompts the user to fix issues (Steps 4–5), while `setup-devcontainer` displays issues as informational and continues into its interactive setup flow which inherently resolves them.

##### Step 0 – Check Base Keys

- Verify all `EXAMPLE_ENV_VALUES` keys exist in both files (respecting conditional requirements)
- If any base keys are missing, flag them — this means the project files are out of date with the current CLI version
- Include the missing base keys in the list presented in Step 4

##### Step 1 – Validate Required Metadata

- Check both files for required metadata (`template_name`, `template_path`, `cli_version`)

- If metadata does not exist:
  - Notify the user: "Project files are missing required metadata and must be regenerated."
  - Present a confirmation prompt:
    - **Default: Yes** (displayed in green) — ask whether to select an existing template or create a new one using the current template creation workflow. Once completed, regenerate both files via `write_project_files()` and continue to Step 2.
    - **Alternative: No** (displayed in red) — display a highly visible warning that continuing may cause the environment to not work as expected. Then launch the IDE without changes.

##### Step 2 – Locate and Validate Template

- Using `template_name` and `template_path` from the metadata, locate the developer template
- If the template cannot be found (deleted, renamed, moved):
  - Exit non-zero with error: "Developer template '<template_name>' not found at '<template_path>'. To fix this, either recreate the template with `cdevcontainer template create <name>` or regenerate project files with `cdevcontainer setup-devcontainer <path>`"
- Run `validate_template()` on the loaded template

##### Step 3 – Compare Against Template

- Compare the contents of both project files against the `containerEnv` values in the developer template
- Identify any key/value pairs from the template that are missing from either file

##### Step 4 – Handle Missing Variables

- If any variables are missing (from base key check or template comparison):
  - Notify the user
  - Clearly display:
    - Which variables are missing
    - Which specific file or files they are missing from
    - What values would be added
  - Warn: "Missing variables may cause the environment to be improperly built. The devcontainer may not function correctly."

##### Step 5 – User Confirmation

- Present a prompt: "Missing variables indicate the devcontainer configuration may need to be upgraded."
- Two options:
  1. **Default:** Update devcontainer configuration and add missing variables
     - Add the missing variables to the appropriate files via `write_project_files()`
     - Read `.devcontainer/catalog-entry.json` to determine which catalog (`catalog_url`) and collection (`name`) the project was set up from
     - If `catalog-entry.json` exists: invoke the catalog pipeline (`clone_catalog_repo(catalog_url)` → find matching collection by `name` → `copy_collection_to_project()`) to replace and upgrade `.devcontainer/` configuration files
     - If `catalog-entry.json` is missing (pre-catalog project): prompt the user to select a catalog source using the same flow as `setup-devcontainer`
     - Display the replacement notification (same as "User Decision: Replace" in `setup-devcontainer`)
     - Require keypress to continue
  2. **Alternative:** Only add the missing variables to the existing files
     - Add missing variables to both files via `write_project_files()`
     - Do not modify `.devcontainer/` configuration files

- After either option: launch the IDE

---

## ARG: template create

### Interactive Creation Flow — Full Prompt Sequence

All prompts use the universal input confirmation pattern (display value, "Is this correct?", re-prompt if no).

1. **AWS config enabled** — Select: `true`, `false`. Default: `true`
2. **Default Git branch** — Text, default: `main`, non-empty
3. **Default Python version** — Text, default: `3.12.9`, non-empty
4. **Developer name** — Text, no default, non-empty
5. **Git provider URL** — Text, default: `github.com`, hostname only (no protocol prefix, must contain at least one dot)
6. **Git authentication method** — Select: `Personal Access Token (default)`, `SSH Key`
7. **Git username** — Text, no default, non-empty
8. **Git email** — Text, no default, non-empty
9. **Git token** — Password field, no default, non-empty — **only if token method selected**
10. **SSH private key path** — File path input with full validation (see "SSH Key Input" in Input Constraints). On success, read the file contents and store them in the template under the key `ssh_private_key`. — **only if SSH method selected**
11. **Extra APT packages** — Text, default: `""`, may be empty
12. **Pager** — Select: `cat`, `less`, `more`, `most`. Default: `cat`
13. **AWS output format** — Select: `json`, `table`, `text`, `yaml`. Default: `json` — **only if AWS enabled**
14. **Host proxy** — Select: `true`, `false`. Default: `false`
15. **Host proxy URL** — Text, no default, must start with `http://` or `https://` — **only if host proxy is `true`**
16. **Additional custom environment variables** — see below
17. **AWS profile map** — see existing AWS profile configuration flow — **only if AWS enabled**

### Host Proxy Configuration

- Prompt: "Does this devcontainer require a host proxy?"
- If `true`:
  - Prompt: "Enter the full host proxy URL" with example `http://host.docker.internal:3128`
  - Validate URL starts with `http://` or `https://`
  - Add to template `containerEnv`:
    - `HOST_PROXY=true`
    - `HOST_PROXY_URL=<user input>`
- If `false`:
  - Add to template `containerEnv`:
    - `HOST_PROXY=false`
    - `HOST_PROXY_URL=` (empty string)

### Additional Custom Environment Variables

- Prompt: "Would you like to add additional environment variables required by your devcontainer or scripts?"
- If yes, enter a loop:
  1. Prompt for variable key (non-empty)
  2. Validate key does not conflict with:
     - Any key in `KNOWN_KEYS` (all `EXAMPLE_ENV_VALUES` keys)
     - Any key already set by a built-in prompt in the current session (e.g., `HOST_PROXY`, `HOST_PROXY_URL`)
     - Any custom key already entered earlier in the same loop
  3. On conflict: "The key '<key>' already exists (<source>). Please enter a different key name." — re-prompt
  4. Prompt for variable value (no constraints, may be empty)
  5. Display the key/value pair and ask: "Is this correct?" (yes/no)
  6. If not correct: re-prompt for key and value
  7. Ask: "Would you like to add another variable?" (yes/no)
  8. Repeat until user answers no
- Add all confirmed key/value pairs to `containerEnv` in the template

---

## ARG: template load

### Current Behavior Being Changed

Currently `template load`:

1. Reads template from `~/.devcontainer-templates/<name>.json`
2. Checks version compatibility
3. Writes template data to `devcontainer-environment-variables.json`
4. Does NOT generate `shell.env`

### New Behavior

1. Read template from `~/.devcontainer-templates/<name>.json`
2. Run `validate_template()` — rejects v1.x templates, validates structure, checks base keys, validates constraints, checks auth consistency
3. Write both `devcontainer-environment-variables.json` and `shell.env` via `write_project_files()`
4. If `GIT_AUTH_METHOD=ssh`: write `.devcontainer/ssh-private-key`
5. If `AWS_CONFIG_ENABLED=true`: write `.devcontainer/aws-profile-map.json`
6. Ensure `.gitignore` entries

---

## ARG: template upgrade

### Purpose

Brings an existing template up to the current CLI version's standards. Used for:

- **Major version upgrades** (e.g., 2.x → 3.x) — adds new required keys, migrates structure
- **Minor version additions** — when a CLI update introduces new required base keys without a major semver bump

### Scope

`template upgrade` modifies **only the template file itself**. It does not modify any project files. Projects that reference the upgraded template will detect the changes on their next `cdevcontainer code` run via the validation steps (Steps 0–5), which will prompt the user to sync.

### Behavior

1. Read template from `~/.devcontainer-templates/<name>.json`
2. Run `validate_template()` — this detects all issues:
   - Missing base keys from `EXAMPLE_ENV_VALUES` — prompt for value or use default
   - Invalid constraint values — prompt for correction
   - Auth method inconsistencies — prompt to resolve
   - Missing or outdated `cli_version`
3. Update `cli_version` to the current CLI version
4. Save the updated template file
5. Display: "Template '<name>' upgraded to CLI v<version>. Projects using this template will be updated on next `cdevcontainer code` run."

### Version Transitions

- **v1.x template:** Rejected by `validate_template()` with: "This template was created with CLI v1.x and is not compatible with v2.0.0. Please recreate your template using `cdevcontainer template create <name>`"
- **v2.x template with missing keys from newer v2.x CLI:** Adds missing keys, updates `cli_version`, saves
- **v2.x template already at current version:** No changes needed — inform user the template is already up to date

---

## Git Authentication

### Two Methods: Token and SSH

#### Token Method (`GIT_AUTH_METHOD=token`)

- `GIT_TOKEN` required in template and project files
- `ssh_private_key` must NOT exist in template
- `.devcontainer/ssh-private-key` must NOT exist in project

**Postcreate script behavior (token):**
- Create `~/.netrc`:
  ```text
  machine <GIT_PROVIDER_URL>
  login <GIT_USER>
  password <GIT_TOKEN>
  ```
- Set `~/.netrc` permissions to 600
- `.gitconfig` includes `[credential] helper = store`

#### SSH Method (`GIT_AUTH_METHOD=ssh`)

- `GIT_TOKEN` must NOT exist in template or project files
- `ssh_private_key` must exist in template
- `.devcontainer/ssh-private-key` written to project (gitignored)

**Postcreate script behavior (SSH):**
1. Ensure `openssh-client` is installed (`apt-get install -y openssh-client`)
2. Create `~/.ssh/` directory with permissions 700
3. Copy `.devcontainer/ssh-private-key` to `~/.ssh/id_private_key`
4. Set `~/.ssh/id_private_key` permissions to 600
5. Run `ssh-keyscan ${GIT_PROVIDER_URL} >> ~/.ssh/known_hosts`
6. Create `~/.ssh/config`:
   ```text
   Host <GIT_PROVIDER_URL>
     HostName <GIT_PROVIDER_URL>
     User git
     IdentityFile ~/.ssh/id_private_key
     IdentitiesOnly yes
   ```
7. Set `~/.ssh/config` permissions to 600
8. Verify SSH connectivity: `ssh -T git@${GIT_PROVIDER_URL}`
   - If fails: exit non-zero with clear error explaining the SSH key may be invalid or not registered with the git provider
9. Do NOT create `~/.netrc`
10. `.gitconfig` does NOT include `[credential] helper = store`

#### Shared Git Config (both methods)

```ini
[user]
    name = <GIT_USER>
    email = <GIT_USER_EMAIL>
[core]
    editor = vim
[push]
    autoSetupRemote = true
[safe]
    directory = *
[pager]
    branch = false
    config = false
    diff = false
    log = false
    show = false
    status = false
    tag = false
```

#### GIT_PROVIDER_URL Usage

The CLI and postcreate script build the correct URL format based on `GIT_AUTH_METHOD`:

- **Token:** `.netrc` `machine` field uses the bare hostname `<GIT_PROVIDER_URL>` (not a URL). Git credential URLs use `https://<GIT_PROVIDER_URL>`.
- **SSH:** `git@<GIT_PROVIDER_URL>` for SSH config and connectivity test

The user only provides the hostname (e.g., `github.com`). The protocol is never stored.

---

## devcontainer.json Changes

### Remove `containerEnv` Block

The entire `containerEnv` section is removed from `devcontainer.json`. All environment variables are sourced from `shell.env` by the `postCreateCommand`.

### Update `postCreateCommand`

The `postCreateCommand` must source `shell.env` **before** all other operations, including `apt-get`:

**Non-WSL:**
```bash
source shell.env && sudo -E apt-get update && sudo -E apt-get install -y gettext-base jq && sudo -E bash .devcontainer/.devcontainer.postcreate.sh vscode
```

**WSL:**
```bash
source shell.env && sudo -E apt-get update && sudo -E apt-get install -y gettext-base jq python3 && find .devcontainer -type f -exec sed -i "s/\r$//" {} + && python3 .devcontainer/fix-line-endings.py && sudo -E apt-get remove -y python3 && sudo -E apt-get autoremove -y && sudo -E bash .devcontainer/.devcontainer.postcreate.sh vscode
```

**Key changes:**
- `source shell.env` at the beginning of both paths replaces the previous pattern where it was sourced after `apt-get install`. This ensures all environment variables (including proxy settings) are available for every operation.
- `sudo -E` preserves the caller's environment variables (including `HTTP_PROXY`, `HTTPS_PROXY`) through `sudo`. Without `-E`, `sudo` strips these variables and `apt-get` behind a corporate proxy would fail.
- The postcreate script no longer needs to re-source `shell.env` separately since it inherits the environment from the `postCreateCommand`.

---

## Postcreate Script Changes

### Environment Configuration

The postcreate script continues to configure `shell.env` sourcing for future shell sessions:

- Append `source "${WORK_DIR}/shell.env"` to `.bashrc`
- Set `BASH_ENV="${WORK_DIR}/shell.env"` in `.bashrc` — this ensures non-interactive bash shells (e.g., scripts, subshells) also source the environment. The same `BASH_ENV` value is also exported in `shell.env` itself (see "Static Container Values") so it is available from the first `postCreateCommand` run before `.bashrc` exists.
- Write `source "${WORK_DIR}/shell.env"` to `.zshenv`

### CICD Handling

- `CICD` is not in `shell.env` or project files
- Postcreate reads it from the runtime environment: `CICD_VALUE="${CICD:-false}"`
- CI pipelines set `CICD=true` in their environment before container creation

### Git Configuration

- Branch on `GIT_AUTH_METHOD` value (see "Git Authentication" section)
- If `GIT_AUTH_METHOD` is unset: exit non-zero with error — "GIT_AUTH_METHOD is required. Please regenerate project files with `cdevcontainer setup-devcontainer <path>`"

### Host Proxy Validation

When `HOST_PROXY=true`, the postcreate script validates that the host proxy is reachable before proceeding with the build. Validation is platform-aware:

**macOS / Linux (nix-family):**
- Validate that the host proxy is reachable at `host.docker.internal:3128` using `nc`
- Wait up to a configurable timeout (default 10 seconds) with active polling
- On failure: exit non-zero with a clear error directing the developer to start tinyproxy on their host and referencing the `nix-family-os/README.md` documentation
- On success: log confirmation and continue

**Windows (WSL):**
- Validate that the host proxy is reachable from within the WSL-based devcontainer
- Same readiness detection approach: active polling with configurable timeout, no sleep-based delays
- On failure: exit non-zero with a clear error directing the developer to start the proxy on their Windows host and referencing the `wsl-family-os/README.md` documentation
- On success: log confirmation and continue

**Host-side proxy toolkits** are provided as collection-specific files:

- `nix-family-os/` — tinyproxy daemon management for macOS/Linux: configuration file, start/stop/restart/status script, LaunchAgent template for auto-start on boot, and comprehensive documentation
- `wsl-family-os/` — proxy management for Windows/WSL: configuration, management script, and documentation matching the macOS/Linux toolkit's capabilities

Both toolkits are included in the `default` collection (and any collection that supports proxy). They are copied into the project's `.devcontainer/` directory by the catalog pipeline and are available for developers to use on their host machines.

When `HOST_PROXY=false`, skip validation and log that proxy is not enabled.

---

## Environment Variable Validation — Two-Stage

### Stage 1: Base Keys (`EXAMPLE_ENV_VALUES`)

Check that all `EXAMPLE_ENV_VALUES` keys exist in both `devcontainer-environment-variables.json` and `shell.env` (respecting conditional requirements). Only key names matter — values in `EXAMPLE_ENV_VALUES` are examples, not expected values.

If any base keys are missing: this likely means a newer CLI version introduced new required variables.

### Stage 2: Developer Template

Locate the developer template using `template_name` and `template_path` metadata from the project files.

- If the template cannot be found: exit non-zero with a clear error explaining the template is missing, what this means, and how to resolve it (recreate template or regenerate project files)
- If found: compare all `containerEnv` keys from the template against both project files. Flag any missing keys with their expected values.

Both stages feed into the shared "Handle Missing Variables" flow (Steps 4–5).

---

## Work Unit 1 Acceptance Criteria

- [ ] All DRY refactoring consolidations implemented (sections A–M)
- [ ] Universal input confirmation on all prompts
- [ ] `validate_template()` shared function with full structural, key, and auth validation
- [ ] `write_project_files()` generates both JSON and shell.env together with metadata
- [ ] `setup-devcontainer` uses catalog pipeline for `.devcontainer/` file source
- [ ] `setup-devcontainer` file validation is informational only (no action prompts)
- [ ] `code` command does not source or generate `shell.env` by default
- [ ] `code --regenerate-shell-env` regenerates shell.env from existing JSON
- [ ] `code` validation Steps 0–5 detect and prompt for missing variables
- [ ] `template create` full prompt sequence with all constraints
- [ ] `template load` generates both files via `write_project_files()`
- [ ] `template upgrade` modifies template only, projects sync on next `code` run
- [ ] Token and SSH git authentication paths fully implemented
- [ ] `containerEnv` removed from `devcontainer.json`
- [ ] `postCreateCommand` sources `shell.env` first, uses `sudo -E`
- [ ] `-y`/`--yes` flag removed, `env` subcommand removed, `install`/`uninstall` removed
- [ ] `bin/cdevcontainer` removed
- [ ] Python `>=3.10` in `pyproject.toml`
- [ ] `NO_PROXY` simplified to `localhost,127.0.0.1,.local`
- [ ] All v1.x templates rejected with clear migration error
- [ ] Two-stage environment variable validation implemented
- [ ] All unit and functional tests pass

---
---

# Work Unit 2: Remote DevContainer Catalog Support

## Overview

Refactor `setup-devcontainer` to use a unified catalog-based architecture for all devcontainer configurations. Every source of devcontainer files — including this repo's default — follows the same catalog structure. The CLI has one code path for discovering, validating, selecting, and copying devcontainer collections regardless of source.

Additionally, create the first specialized catalog repo at Caylent Solutions and provide comprehensive documentation on how to create and maintain catalog repos.

---

## Architecture

### Unified Catalog Model

All devcontainer configuration sources follow the same catalog repo structure. The CLI always processes repos as catalogs — there is no special-casing for "default" vs "specialized."

| Source | URL | When Used |
|---|---|---|
| **Default catalog** (this repo) | Hardcoded as `DEFAULT_CATALOG_URL` constant in CLI | When `DEVCONTAINER_CATALOG_URL` is not set, or when user explicitly chooses "Default" |
| **Specialized catalog** (remote repo) | Read from `DEVCONTAINER_CATALOG_URL` | When ENV var is set and user chooses to browse catalog |

Both go through identical processing: `clone_catalog_repo()` → `discover_collections()` → select → `copy_collection_to_project()`.

### ENV Var: `DEVCONTAINER_CATALOG_URL`

- Set on the developer's host machine (not inside the devcontainer)
- Format: `<git-clone-url>` with optional `@<branch-or-tag>` suffix
- Examples:
  - `https://github.com/caylent-solutions/smarsh-devcontainer-catalog.git` — uses default branch
  - `https://github.com/caylent-solutions/smarsh-devcontainer-catalog.git@v2.0` — uses tag `v2.0`
  - `https://github.com/caylent-solutions/smarsh-devcontainer-catalog.git@develop` — uses branch `develop`
  - `git@github.com:caylent-solutions/smarsh-devcontainer-catalog.git@main` — SSH with branch
- If the `@` suffix is omitted, the default branch of the repository is used
- The developer's shell must already be configured with git authentication for the catalog repo URL. This is a separate concern from the developer template's `GIT_AUTH_METHOD` — the catalog repo may be on a different git provider or require different credentials.

### Authentication Failure Handling

If cloning any catalog repo fails:

- Exit non-zero with a clear error:
  - "Failed to clone the devcontainer catalog from '<url>'. Ensure your shell is configured with git authentication for this repository."
  - "For HTTPS repos: verify a valid token or credential helper is configured."
  - "For SSH repos: verify your SSH key is loaded and the host is in known_hosts."
  - "You can test access by running: `git ls-remote <url>`"
- Do NOT fall back silently to a different source

---

## Catalog Repo Structure

### Required Layout

Every catalog repo — including this repo — must follow this structure:

```
catalog-repo/
  common/
    devcontainer-assets/
      .devcontainer.postcreate.sh      # Shared postcreate script (all collections use this)
      devcontainer-functions.sh         # Shared functions (all collections use this)
      project-setup.sh                  # Template project-setup script (copied to projects)
  collections/
    <collection-name>/
      catalog-entry.json                # Marker file + metadata (required)
      devcontainer.json                 # DevContainer configuration (required)
      ...                               # Other collection-specific files
```

Collections can be nested at any depth under any directory in the repo. The CLI discovers them by recursively scanning for `catalog-entry.json` marker files. The `collections/` directory name is a convention, not a requirement — marker files are found regardless of parent directory names.

### `common/devcontainer-assets/` (Required)

This directory must exist at the repo root and contain at minimum:

| File | Purpose |
|---|---|
| `.devcontainer.postcreate.sh` | Shared postcreate script — all collections use this |
| `devcontainer-functions.sh` | Shared shell functions — all collections use this |
| `project-setup.sh` | Template project-setup script — copied into projects, developer maintains after initial setup |

Additional files may exist in `common/devcontainer-assets/` (e.g., shared configuration templates). All files in this directory are considered "common assets."

### `catalog-entry.json` (Marker File + Metadata)

Each collection directory must contain a `catalog-entry.json` file. This file serves as both a discovery marker and metadata source.

```json
{
  "name": "java-spring-boot",
  "description": "Java 21 with Spring Boot 3.x, Gradle, and Checkstyle. Includes Maven support and Spring-specific VS Code extensions.",
  "tags": ["java", "spring-boot", "gradle", "backend"],
  "maintainer": "Platform Team",
  "min_cli_version": "2.0.0"
}
```

#### Required Fields

| Field | Type | Constraints | Description |
|---|---|---|---|
| `name` | string | Lowercase, dash-separated, no spaces, no special characters except dashes. Minimum 2 characters. Must match `^[a-z][a-z0-9-]*[a-z0-9]$`. Must be unique across the entire catalog. | Display name and selection identifier |
| `description` | string | Non-empty | Human-readable description of what the devcontainer provides |

#### Optional Fields

| Field | Type | Description |
|---|---|---|
| `tags` | array of strings | Categories for grouping and filtering (lowercase, dash-separated) |
| `maintainer` | string | Team or person responsible for this collection |
| `min_cli_version` | string | Minimum CLI version required to use this collection (semver) |

### Collection Contents

Each collection directory must contain:

- `catalog-entry.json` — marker + metadata (required)
- `devcontainer.json` — devcontainer configuration (required)
- `VERSION` — version identifier for the collection (required). Used by `setup-devcontainer` to display the current version when detecting existing configuration. Plain text file containing a version string (e.g., `1.0.0`).

Each collection directory may also contain:

- Additional configuration files, scripts, or directories specific to that environment
- Examples: linter configs, VS Code workspace settings, Dockerfile, docker-compose files

Each collection directory must NOT contain:

- Any file with the same name as a file in `common/devcontainer-assets/`
- This prevents conflicts when both common assets and collection files are copied into a project's `.devcontainer/`
- The validation command enforces this rule

### `postCreateCommand` Standardization

Every collection's `devcontainer.json` must contain a `postCreateCommand` that calls the shared postcreate script from `common/devcontainer-assets/`. The exact command must follow the same pattern defined in the "devcontainer.json Changes" section in Work Unit 1 — sources `shell.env` first, uses `sudo -E` for proxy support.

The validation command enforces that the `postCreateCommand` in every collection's `devcontainer.json` calls `.devcontainer/.devcontainer.postcreate.sh`.

### `project-setup.sh` Lifecycle

1. `common/devcontainer-assets/project-setup.sh` is the template version
2. When `setup-devcontainer` copies files to a project, `project-setup.sh` is included
3. The shared postcreate script calls `project-setup.sh` if it exists: `if [ -f "${WORK_DIR}/.devcontainer/project-setup.sh" ]; then source "${WORK_DIR}/.devcontainer/project-setup.sh"; fi`
4. The developer edits `project-setup.sh` after initial setup for project-specific customization
5. On re-setup (when the user chooses to replace `.devcontainer/` files), `project-setup.sh` is overwritten along with all other devcontainer files. The existing replacement notification (from "User Decision: Replace" in Work Unit 1) already instructs the developer to merge back project-specific customizations and test before pushing.

---

## Refactoring This Repo to Catalog Structure

### New Structure

The catalog structure is added alongside the existing repo contents. `.devcontainer/` is **untouched** — it is this repo's own development environment for working on the CLI and is completely independent of the catalog.

```
.devcontainer/                          <- UNCHANGED — this repo's own dev environment (NOT part of the catalog)
caylent-devcontainer-cli/               <- CLI source code
claude-backlog/                         <- Backlog specs
common/                                 <- NEW — shared catalog assets
  devcontainer-assets/
    .devcontainer.postcreate.sh         <- distributable postcreate script
    devcontainer-functions.sh           <- distributable shared functions
    project-setup.sh                    <- distributable template project-setup script
collections/                            <- NEW — catalog collections
  default/
    catalog-entry.json                  <- metadata marker
    devcontainer.json                   <- distributable devcontainer configuration
    nix-family-os/                      <- macOS/Linux proxy toolkit
    wsl-family-os/                      <- Windows/WSL proxy toolkit
    fix-line-endings.py
    example-container-env-values.json
    example-aws-profile-map.json
    ...other collection-specific files
CLAUDE.md
...
```

**Key points:**
- `.devcontainer/` is NOT part of the catalog. It exists solely for developing this repo and is independently maintained.
- `common/devcontainer-assets/` and `collections/default/` are the distributable catalog content.
- The files in `common/` and `collections/default/` may differ from the files in `.devcontainer/` — they serve different purposes (distribution vs. CLI development).
- When the CLI clones this repo as the default catalog, it scans for `catalog-entry.json` marker files. It finds `collections/default/catalog-entry.json` and ignores everything else (`.devcontainer/`, `caylent-devcontainer-cli/`, etc.).

### `collections/default/catalog-entry.json`

```json
{
  "name": "default",
  "description": "General-purpose Caylent development environment with Python, Node.js, Java, AWS CLI, Docker, and Kubernetes tools.",
  "tags": ["general", "multi-language", "aws", "kubernetes"],
  "maintainer": "Caylent Platform Team",
  "min_cli_version": "2.0.0"
}
```

### CLI Constant

Add to `utils/constants.py`:

```python
DEFAULT_CATALOG_URL = "https://github.com/caylent-solutions/devcontainer.git"
```

The CLI uses this URL when `DEVCONTAINER_CATALOG_URL` is not set. Both URLs are processed by the same `clone_catalog_repo()` function.

---

## CLI Changes

### New Subcommand: `cdevcontainer catalog`

#### `cdevcontainer catalog list`

Lists all available collections from a catalog repo.

**Behavior:**

1. Determine catalog URL:
   - If `DEVCONTAINER_CATALOG_URL` is set: use it
   - If not set: use `DEFAULT_CATALOG_URL`
2. Parse URL and optional branch/tag
3. Clone the catalog repo to a temporary directory (shallow clone: `git clone --depth 1 --branch <ref>` or `git clone --depth 1` for default branch)
4. Validate the catalog structure (common assets exist, at least one collection)
5. Scan recursively for `catalog-entry.json` marker files
6. Parse metadata from each marker file
7. Sort by name ascending (A-Z), with `default` first if it exists
8. Display as a flat list:
   ```
   Available DevContainer Configurations (<source>):

     default             General-purpose Caylent development environment
     airflow-data-eng    Apache Airflow with Python 3.12 for data engineering pipelines
     java-spring-boot    Java 21 with Spring Boot 3.x, Gradle, and Checkstyle
     node-angular        Node.js 22 with Angular 19 and ESLint
     python-fastapi      Python 3.12 with FastAPI, PostgreSQL, and Redis
   ```
   Where `<source>` is "default catalog" or the ENV var URL.
9. Clean up temporary directory

**Flags:**

- `--tags <tag1,tag2>` — filter by tags (show only entries matching ANY of the provided tags). If no collections match the filter, display: "No collections found matching tags: <tags>"

**Note:** When using the default catalog (no `DEVCONTAINER_CATALOG_URL` set), the list will show only the single `default` collection. The multi-collection display example above applies when using specialized catalogs with multiple collections.

#### `cdevcontainer catalog validate`

Validates a catalog repo for structural compliance.

**Two modes:**

- `cdevcontainer catalog validate` — clones and validates. Uses `DEVCONTAINER_CATALOG_URL` if set, otherwise `DEFAULT_CATALOG_URL`.
- `cdevcontainer catalog validate --local <path>` — validates a local directory (for catalog repo maintainers working on a checked-out repo)

**Validation checks:**

1. **Common assets exist:**
   - `common/devcontainer-assets/` directory exists
   - `.devcontainer.postcreate.sh` exists in common assets
   - `devcontainer-functions.sh` exists in common assets
   - `project-setup.sh` exists in common assets

2. **Collection discovery:**
   - Scan recursively for `catalog-entry.json` files
   - If none found: error "No collections found in catalog"

3. **Per-collection validation:**
   - `catalog-entry.json` is valid JSON
   - Required fields exist: `name`, `description`
   - `name` matches pattern `^[a-z][a-z0-9-]*[a-z0-9]$` — lowercase, dash-separated, no spaces, no special characters
   - `devcontainer.json` exists in the collection directory
   - `devcontainer.json` is valid JSON
   - `postCreateCommand` in `devcontainer.json` calls `.devcontainer/.devcontainer.postcreate.sh`
   - `min_cli_version` (if present) is valid semver

4. **No file conflicts with common assets:**
   - For every file in `common/devcontainer-assets/`, verify no collection directory contains a file with the same name (at any nesting level within the collection)
   - Report each conflict: "Collection '<name>' contains '<filename>' which conflicts with common/devcontainer-assets/<filename>"

5. **Unique names:**
   - All collection `name` values must be unique across the entire catalog
   - Report duplicates: "Duplicate collection name '<name>' found at '<path1>' and '<path2>'"

6. **Tags validation (if present):**
   - Tags must be lowercase, dash-separated strings

**Output:**

- On success: "Catalog validation passed. <N> collections found."
- On failure: list all violations, then exit non-zero with: "Catalog validation failed. <N> issues found."

---

### Changes to `setup-devcontainer`

#### Unified Flow (Always Catalog-Based)

`setup-devcontainer` always uses the catalog pipeline. There is no separate "copy `.devcontainer/`" path.

**When `DEVCONTAINER_CATALOG_URL` is NOT set:**

1. Clone the default catalog (this repo) using `DEFAULT_CATALOG_URL`
2. Discover collections — the default catalog has one collection: `default`
3. Since there is only one collection, select it automatically (no browsing UI needed)
4. Copy collection files + common assets to project's `.devcontainer/`
5. Continue with interactive setup (template selection/creation, env var prompts)

**When `DEVCONTAINER_CATALOG_URL` IS set:**

1. Present a source selection prompt:
   ```
   DevContainer configuration sources available:

   > Default Caylent General DevContainer
     Browse specialized configurations from catalog
   ```
   User navigates with arrow keys.

2. **If "Default" selected:**
   - Clone the default catalog using `DEFAULT_CATALOG_URL`
   - Discover collections, auto-select `default`
   - Copy and continue with interactive setup

3. **If "Browse catalog" selected:**
   a. Clone the specialized catalog from `DEVCONTAINER_CATALOG_URL`
   b. Validate catalog structure (common assets exist, at least one collection)
   c. Discover collections, parse metadata
   d. Build a flat list sorted by name (A-Z), with `default` first if it exists
   e. Present a searchable/scrollable selection list using `questionary`:
      ```
      Select a devcontainer configuration:

      > default             General-purpose development environment
        airflow-data-eng    Apache Airflow with Python 3.12 for data engineering pipelines
        java-spring-boot    Java 21 with Spring Boot 3.x, Gradle, and Checkstyle
        node-angular        Node.js 22 with Angular 19 and ESLint
        python-fastapi      Python 3.12 with FastAPI, PostgreSQL, and Redis
      ```
      - Arrow keys to navigate
      - Type to filter/search by name
      - Enter to select
   f. Display the full metadata (name, description, tags, maintainer) and ask: "Is this correct?" (universal input confirmation)
   g. Copy collection files + common assets to project's `.devcontainer/`
   h. Continue with interactive setup

4. Clean up temporary directory (all paths)

**In all paths**, the file copy step is the same function: `copy_collection_to_project(collection_path, common_assets_path, target_path)`.

#### `--catalog-entry <name>` Flag

Shortcut to skip the browsing UI when the user already knows the collection name:

```
cdevcontainer setup-devcontainer --catalog-entry java-spring-boot <path>
```

**Behavior:**

1. Requires `DEVCONTAINER_CATALOG_URL` to be set — exit non-zero if not
2. Clone the specialized catalog from `DEVCONTAINER_CATALOG_URL`
3. Find collection with matching `name` — exit non-zero if not found: "Collection '<name>' not found in catalog. Run 'cdevcontainer catalog list' to see available collections."
4. Skip the source selection prompt and browsing UI
5. Display the collection metadata and ask: "Is this correct?" (universal input confirmation)
6. Copy collection files + common assets, continue with interactive setup

#### File Copy Behavior

Single function `copy_collection_to_project(collection_path, common_assets_path, target_path, catalog_url)`:

1. Copy ALL files from the collection directory into the project's `.devcontainer/` — including `catalog-entry.json`
2. Augment the copied `.devcontainer/catalog-entry.json` by adding a `catalog_url` field set to the URL the CLI used to clone the catalog. This stamps the source of origin into the project so that future `code` Step 5 (Work Unit 1) can re-fetch from the same catalog. The augmented file looks like:
   ```json
   {
     "name": "java-spring-boot",
     "description": "Java 21 with Spring Boot 3.x...",
     "tags": ["java", "spring-boot"],
     "maintainer": "Platform Team",
     "min_cli_version": "2.0.0",
     "catalog_url": "https://github.com/caylent-solutions/smarsh-devcontainer-catalog.git"
   }
   ```
3. Copy ALL files from `common/devcontainer-assets/` into the project's `.devcontainer/`
4. Since validation ensures no filename conflicts between common assets and collection files, these operations will not overwrite each other. (`catalog-entry.json` exists only in collections, not in common assets.)
5. Call `remove_example_files()` (from Work Unit 1 Section L) to clean up example JSON files from the project's `.devcontainer/`
6. The replacement notification and acknowledgement flow (from "User Decision: Replace" in Work Unit 1) applies
7. Continue with interactive setup (template selection/creation, env var prompts, `write_project_files()`)

---

## Shared Functions

The catalog logic must be implemented as shared functions reused by all consumers. This follows the DRY requirements in Work Unit 1.

- **`parse_catalog_url(url_with_ref) -> (clone_url, ref)`** — Parses URL + optional ref. See "URL Parsing" section.
- **`clone_catalog_repo(url_with_ref) -> temp_path`** — Calls `parse_catalog_url()`, performs shallow clone to temp dir, returns path. Handles auth errors with actionable messages.
- **`validate_catalog_structure(repo_path) -> list[ValidationError]`** — Validates common assets exist.
- **`discover_collections(repo_path) -> list[CollectionEntry]`** — Recursively scans for `catalog-entry.json`, parses metadata, returns sorted list with `default` first.
- **`validate_collection(collection_path, common_assets_path) -> list[ValidationError]`** — Validates a single collection (metadata, conflicts, postCreateCommand).
- **`validate_catalog(repo_path) -> list[ValidationError]`** — Full validation: structure + all collections + uniqueness.
- **`copy_collection_to_project(collection_path, common_assets_path, target_path, catalog_url)`** — Copies collection + common assets into target `.devcontainer/`. Augments `catalog-entry.json` with `catalog_url` before writing.

---

## URL Parsing

The `DEVCONTAINER_CATALOG_URL` value is parsed as:

```
<clone-url>[@<ref>]
```

Where:
- `<clone-url>` is a valid git clone URL (HTTPS or SSH)
- `<ref>` is an optional branch name or tag name
- The `@` delimiter is only interpreted as a ref separator if it appears after the `.git` suffix or at the end of the URL path. For SSH URLs like `git@github.com:org/repo.git@v2.0`, the parser must distinguish between the SSH user `git@` and the ref delimiter `@v2.0`.

**Parsing rules:**
- If URL ends with `.git@<ref>`: split on last `@` after `.git`
- If URL ends with `.git`: no ref, use default branch
- If URL has no `.git` suffix: split on last `@` if present, otherwise use default branch

**Examples:**

| Input | Clone URL | Ref |
|---|---|---|
| `https://github.com/org/repo.git` | `https://github.com/org/repo.git` | (default branch) |
| `https://github.com/org/repo.git@v2.0` | `https://github.com/org/repo.git` | `v2.0` |
| `https://github.com/org/repo.git@develop` | `https://github.com/org/repo.git` | `develop` |
| `git@github.com:org/repo.git` | `git@github.com:org/repo.git` | (default branch) |
| `git@github.com:org/repo.git@v2.0` | `git@github.com:org/repo.git` | `v2.0` |

---

## Impact on Existing Commands

### `code` Command

No changes to the `code` command's normal flow — it operates on project files that already exist and does not care whether they came from the default catalog or a specialized catalog collection. However, when `code` Step 5 option 1 (from Work Unit 1) needs to replace `.devcontainer/` files, it reads `.devcontainer/catalog-entry.json` to determine the `catalog_url` and collection `name`, then invokes the catalog pipeline to fetch and copy updated files. This `catalog-entry.json` is committed to the project repo by `copy_collection_to_project()` during `setup-devcontainer`.

### `template create` / `template load`

No changes. Templates store environment variables and developer preferences. They are independent of which devcontainer configuration was used. A developer can use the same template across projects with different catalog entries.

### `setup-devcontainer`

Fully refactored to use the unified catalog pipeline as described above. The previous "clone this repo, copy `.devcontainer/`" logic is replaced with "clone catalog repo, discover collections, select, copy collection + common assets."

---

## Specialized Catalog Repos

Two specialized catalog repos are created alongside the default catalog (this repo).

### Repository: `caylent-solutions/caylent-devcontainer-catalog`

The Caylent internal catalog serves all Caylent engineering practices: CAE, CNA, CDE, and Solutions. It follows the same structure as this repo's catalog layout.

#### Initial Structure

```
caylent-devcontainer-catalog/
  README.md                             <- Comprehensive catalog documentation
  CONTRIBUTING.md                       <- How to add/maintain collections
  common/
    devcontainer-assets/
      .devcontainer.postcreate.sh       <- Shared postcreate (can differ from default catalog)
      devcontainer-functions.sh         <- Shared functions
      project-setup.sh                  <- Template project-setup
  collections/
    (initial collections TBD based on practice needs)
```

The Caylent internal catalog has its OWN `common/devcontainer-assets/` — these shared scripts are independent of the default catalog's shared scripts. Each catalog is self-contained.

### Repository: `caylent-solutions/smarsh-devcontainer-catalog`

The Smarsh client catalog is built for the Smarsh client engagement. It follows the same structure as this repo's catalog layout.

#### Initial Structure

```
smarsh-devcontainer-catalog/
  README.md                             <- Comprehensive catalog documentation
  CONTRIBUTING.md                       <- How to add/maintain collections
  common/
    devcontainer-assets/
      .devcontainer.postcreate.sh       <- Shared postcreate (can differ from default catalog)
      devcontainer-functions.sh         <- Shared functions
      project-setup.sh                  <- Template project-setup
  collections/
    smarsh-java-backend/
      catalog-entry.json
      devcontainer.json
      ...                               <- Java/Spring Boot backend collection files
    smarsh-angular-fullstack/
      catalog-entry.json
      devcontainer.json
      ...                               <- Angular + backend fullstack collection files
```

The Smarsh catalog has its OWN `common/devcontainer-assets/` — these shared scripts are independent of the default catalog's and the Caylent internal catalog's shared scripts. Each catalog is self-contained.

### Collections

#### `smarsh-java-backend`

Java backend services for Smarsh. Targets deployment on Amazon EKS within the Smarsh environment.

```json
{
  "name": "smarsh-java-backend",
  "description": "Java 21 with Spring Boot 3.x, Gradle, and Checkstyle for Smarsh backend services. Targets EKS deployment.",
  "tags": ["java", "spring-boot", "gradle", "backend", "eks", "smarsh"],
  "maintainer": "Caylent Platform Team",
  "min_cli_version": "2.0.0"
}
```

**Scope:**
- Java 21, Spring Boot 3.x, Gradle build system
- Checkstyle and code quality tooling
- Spring-specific VS Code extensions
- Kubernetes and Helm tooling for EKS deployment workflows
- AWS CLI and EKS-specific configuration
- Backend-only — no frontend tooling or extensions

#### `smarsh-angular-fullstack`

Angular frontend with Java backend for Smarsh fullstack services. Targets deployment on Amazon EKS within the Smarsh environment.

```json
{
  "name": "smarsh-angular-fullstack",
  "description": "Angular 19 with Node.js 22, Java 21, and Spring Boot 3.x for Smarsh fullstack services. Targets EKS deployment.",
  "tags": ["angular", "node", "java", "spring-boot", "fullstack", "eks", "smarsh"],
  "maintainer": "Caylent Platform Team",
  "min_cli_version": "2.0.0"
}
```

**Scope:**
- Everything in `smarsh-java-backend` plus:
- Node.js 22 with npm for Angular builds
- Angular CLI and Angular-specific VS Code extensions
- ESLint for TypeScript/JavaScript
- Frontend debug port forwarding (4200 for Angular dev server, 9229 for Node.js debug)
- Both frontend and backend tooling in a single devcontainer

#### Shared Characteristics (Both Collections)

- Both collections target **Amazon EKS** for deployment — Kubernetes and Helm tooling included
- Both are **Smarsh-specific** — the catalog is intended for Smarsh project teams only
- Both use the same `common/devcontainer-assets/` shared scripts (postcreate, functions, project-setup)
- Smarsh developers set `DEVCONTAINER_CATALOG_URL` to point to this catalog repo and select the appropriate collection for their project type

---

## Documentation: How to Create and Maintain Catalog Repos

### Location

Documentation lives in two places:

1. **In the Caylent internal catalog repo** (`caylent-devcontainer-catalog/README.md` and `CONTRIBUTING.md`) — the canonical reference
2. **In this repo** — a brief reference pointing to the catalog documentation

### README.md Contents (Comprehensive)

#### What is a DevContainer Catalog?

- Explanation of the catalog model
- Relationship between catalogs, collections, common assets, and developer templates
- How the CLI discovers and uses catalogs

#### Catalog Repo Structure

- Full directory layout with explanations
- `common/devcontainer-assets/` — what each file does, why they are shared
- `catalog-entry.json` — full schema reference with all fields
- `devcontainer.json` — requirements and constraints (postCreateCommand standardization)

#### Creating a New Catalog Repo

Step-by-step guide:

1. Create a new git repository
2. Create `common/devcontainer-assets/` with the three required files:
   - `.devcontainer.postcreate.sh` — explain what this script does and how to customize it for the catalog's needs
   - `devcontainer-functions.sh` — explain the shared function library
   - `project-setup.sh` — explain its purpose as a project-level hook
3. Create your first collection directory with `catalog-entry.json` and `devcontainer.json`
4. Run `cdevcontainer catalog validate --local .` to verify compliance
5. Push to your git provider

#### Adding a New Collection

Step-by-step guide:

1. Create a directory (can be nested at any level)
2. Create `catalog-entry.json` with required metadata:
   - Name constraints: lowercase, dash-separated, unique, matches `^[a-z][a-z0-9-]*[a-z0-9]$`
   - Description: clear, concise explanation of what the devcontainer provides
   - Tags: relevant categories for filtering
3. Create `devcontainer.json` with appropriate features, extensions, and settings
   - MUST include the standardized `postCreateCommand` that calls the shared postcreate script
   - MUST NOT duplicate files from `common/devcontainer-assets/`
4. Add any collection-specific files (linter configs, Dockerfiles, etc.)
5. Run `cdevcontainer catalog validate --local .` to verify:
   - No filename conflicts with common assets
   - Name is unique
   - Metadata is valid
   - postCreateCommand is compliant
6. Commit and push

#### Modifying Common Assets

- Explain that changes to `common/devcontainer-assets/` affect ALL collections
- Recommend thorough testing before modifying shared scripts
- Suggest versioning/tagging the catalog repo for stability

#### postCreateCommand Reference

- Exact format required in every `devcontainer.json`
- Non-WSL and WSL variants
- Why `source shell.env` must come first
- Why `sudo -E` is required for proxy support

#### Customization Model

Explain the three layers of customization:

1. **Catalog collections** — devcontainer configuration (features, extensions, base image)
2. **Developer templates** — environment variables and personal preferences (managed by `cdevcontainer template create`)
3. **project-setup.sh** — project-specific setup scripts (developer-maintained after initial setup)

#### Validation Reference

- Full list of validation checks with error messages
- How to run `cdevcontainer catalog validate --local .`
- Common validation failures and how to fix them

#### Distributing Your Catalog

- Set `DEVCONTAINER_CATALOG_URL` on developer machines
- Branch/tag support for version pinning
- Authentication requirements (HTTPS vs SSH)

### CONTRIBUTING.md Contents

- Pull request process for adding new collections
- Required validation before merge
- Naming conventions
- Testing expectations
- Review checklist

---

## Catalog Error Handling

| Scenario | Behavior |
|---|---|
| `DEVCONTAINER_CATALOG_URL` not set + `--catalog-entry` | Exit non-zero: "DEVCONTAINER_CATALOG_URL is not set. The --catalog-entry flag requires a specialized catalog. Set this environment variable to the git URL of your devcontainer catalog repository." |
| Clone fails (auth, network, invalid URL) | Exit non-zero with actionable message (see "Authentication Failure Handling") |
| Catalog repo has no `common/devcontainer-assets/` | Exit non-zero: "Catalog repo is missing required directory 'common/devcontainer-assets/'. The catalog maintainer must add this directory with the required shared scripts." |
| Catalog repo has no collections | Exit non-zero: "No devcontainer collections found in the catalog. The catalog maintainer must add at least one collection with a catalog-entry.json marker file." |
| `--catalog-entry <name>` not found | Exit non-zero: "Collection '<name>' not found in catalog. Run 'cdevcontainer catalog list' to see available collections." |
| Collection missing `devcontainer.json` | Validation error — skip this collection in list/browse, report in `catalog validate` |
| `min_cli_version` higher than current CLI | Warn: "Collection '<name>' requires CLI v<version> or higher. You are running v<current>." Skip in list/browse unless forced. |

---

## Catalog Testing Requirements

### Unit Tests

- URL parsing (all formats, edge cases with `@` in SSH URLs)
- `catalog-entry.json` metadata validation (valid, missing fields, invalid name patterns)
- Collection discovery (nested directories, mixed valid/invalid collections, `default` sorting)
- File conflict detection (matching filenames at various nesting levels within collections)
- Name uniqueness validation
- `postCreateCommand` validation
- Tag filtering logic
- `copy_collection_to_project()` correctness (collection files + common assets merged, `catalog-entry.json` copied and augmented with `catalog_url`)

### Functional Tests

- End-to-end `catalog list` with default catalog (this repo)
- End-to-end `catalog list` with `DEVCONTAINER_CATALOG_URL` pointing to a test catalog
- End-to-end `catalog validate` against valid and invalid catalog repos
- `catalog validate --local` against a local directory
- `setup-devcontainer` without ENV var — auto-selects default collection, single code path
- `setup-devcontainer` with ENV var — source selection prompt, browsing UI
- `setup-devcontainer --catalog-entry <name>` — direct selection flow
- File copy correctness — common assets + collection files merged into `.devcontainer/`
- Authentication failure handling (invalid URL, unreachable host)
- Interaction with existing template flow after catalog selection
- `project-setup.sh` overwrite behavior on re-setup

---

## Work Unit 2 Acceptance Criteria

- [ ] Catalog structure added to this repo (`common/devcontainer-assets/`, `collections/default/`) — `.devcontainer/` untouched
- [ ] `DEFAULT_CATALOG_URL` constant in CLI points to this repo
- [ ] `setup-devcontainer` uses unified catalog pipeline for all sources — one code path
- [ ] Without ENV var: auto-clones this repo, auto-selects `default` collection
- [ ] With ENV var: presents source selection, browsing UI for specialized catalog
- [ ] `cdevcontainer catalog list` displays collections (from default or ENV var catalog)
- [ ] `cdevcontainer catalog list --tags <tags>` filters by tags
- [ ] `cdevcontainer catalog validate` validates remote catalog
- [ ] `cdevcontainer catalog validate --local <path>` validates local catalog
- [ ] Validation catches: missing common assets, file conflicts, duplicate names, invalid metadata, non-compliant postCreateCommand
- [ ] `--catalog-entry <name>` skips browsing and selects directly
- [ ] `copy_collection_to_project()` correctly merges collection + common assets into `.devcontainer/`
- [ ] `.devcontainer/catalog-entry.json` persists in project repo with `catalog_url` augmented — committed to git, NOT gitignored
- [ ] `project-setup.sh` copied and called by postcreate script as a hook
- [ ] Authentication failures produce actionable error messages
- [ ] All shared logic is implemented once (DRY)
- [ ] Caylent internal catalog repo created at `caylent-solutions/caylent-devcontainer-catalog`
- [ ] Smarsh client catalog repo created at `caylent-solutions/smarsh-devcontainer-catalog`
- [ ] `smarsh-java-backend` collection with Java 21, Spring Boot, Gradle, EKS tooling
- [ ] `smarsh-angular-fullstack` collection with Angular 19, Node.js 22, Java 21, Spring Boot, EKS tooling
- [ ] Comprehensive `README.md` and `CONTRIBUTING.md` in the catalog repo
- [ ] Unit and functional tests cover all scenarios
- [ ] All code follows CLAUDE.md standards

---
---

# Work Unit 3: Mirror DevContainer Base Image to ECR Public

## Problem

The Microsoft devcontainer base image (`mcr.microsoft.com/devcontainers/base:noble`) is hosted on Azure, which lacks points of presence in South America. This causes slow download times for developers in that region.

## Solution

Mirror the image to Amazon ECR Public (`public.ecr.aws`), which has built-in global CloudFront distribution. Create a GitHub Actions workflow that polls for new upstream images on a schedule and publishes them to ECR Public.

---

## Phase 1: ECR Public Infrastructure Setup

Set up via Claude agentic agent using the developer's authenticated AWS CLI session from this devcontainer.

### ECR Public Repository

- Create an ECR Public repository in `us-east-1` (ECR Public is only available in us-east-1)
- Repository name: `caylent-solutions/devcontainer-base` (or as appropriate for Caylent naming conventions)
- Configure repository catalog data:
  - Description: "Mirror of mcr.microsoft.com/devcontainers/base for global low-latency pulls"
  - Operating systems: Linux
  - Architecture: amd64, arm64

### IAM OIDC Provider for GitHub Actions

- Create an IAM OIDC identity provider for GitHub Actions:
  - Provider URL: `https://token.actions.githubusercontent.com`
  - Audience: `sts.amazonaws.com`
- This enables GitHub Actions to assume IAM roles without long-lived AWS credentials (no `AWS_ACCESS_KEY_ID`/`AWS_SECRET_ACCESS_KEY` stored as GitHub secrets)

### IAM Role for GitHub Actions

- Create an IAM role: `github-actions-ecr-public-push` (or per Caylent naming conventions)
- Trust policy: restrict to this specific GitHub repository and the workflow's branch
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": {
          "Federated": "arn:aws:iam::<ACCOUNT_ID>:oidc-provider/token.actions.githubusercontent.com"
        },
        "Action": "sts:AssumeRoleWithWebIdentity",
        "Condition": {
          "StringEquals": {
            "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
          },
          "StringLike": {
            "token.actions.githubusercontent.com:sub": "repo:caylent-solutions/<REPO_NAME>:ref:refs/heads/main"
          }
        }
      }
    ]
  }
  ```
- Permissions policy (least privilege — ECR Public push only):
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Action": [
          "ecr-public:GetAuthorizationToken",
          "ecr-public:BatchCheckLayerAvailability",
          "ecr-public:InitiateLayerUpload",
          "ecr-public:UploadLayerPart",
          "ecr-public:CompleteLayerUpload",
          "ecr-public:PutImage",
          "ecr-public:DescribeImages",
          "ecr-public:DescribeRepositories"
        ],
        "Resource": "arn:aws:ecr-public::<ACCOUNT_ID>:repository/caylent-solutions/devcontainer-base"
      },
      {
        "Effect": "Allow",
        "Action": [
          "ecr-public:GetAuthorizationToken",
          "sts:GetServiceBearerToken"
        ],
        "Resource": "*"
      }
    ]
  }
  ```

---

## Phase 2: GitHub Actions Workflow

### Workflow: `.github/workflows/mirror-devcontainer-image.yml`

**Trigger:** Scheduled cron semi-monthly (1st and 15th) + manual `workflow_dispatch`

**Steps:**

1. **Check for upstream image update**
   - Pull the manifest digest from `mcr.microsoft.com/devcontainers/base:noble` (using `docker manifest inspect` or `crane digest`)
   - Compare against the digest currently in ECR Public (using `crane digest` against `public.ecr.aws/...`)
   - If digests match: skip — image is already up to date, exit successfully
   - If digests differ or ECR image doesn't exist: continue

2. **Authenticate to ECR Public via OIDC**
   - Use `aws-actions/configure-aws-credentials` with the IAM role ARN (no stored secrets)
   - Authenticate Docker to ECR Public: `aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws`

3. **Pull, tag, and push**
   - Pull `mcr.microsoft.com/devcontainers/base:noble`
   - Tag as `public.ecr.aws/<ALIAS>/devcontainer-base:noble`
   - Also tag with the upstream digest as a version tag for traceability (e.g., `public.ecr.aws/<ALIAS>/devcontainer-base:noble-<short-sha>`)
   - Push both tags

4. **Update devcontainer.json files**
   - Update the `"image"` field in both `.devcontainer/devcontainer.json` (this repo's own dev environment) and `collections/default/devcontainer.json` (the distributable default catalog collection) to reference the ECR Public image
   - Only update if the image reference has changed

5. **Create pull request**
   - If either `devcontainer.json` was updated, create a PR with:
     - Title: "chore: update devcontainer base image to latest mirror"
     - Body: includes upstream digest, date, and link to upstream release notes if available
   - If no changes: no PR created

### Workflow Configuration

```yaml
name: Mirror DevContainer Base Image

on:
  schedule:
    - cron: '0 6 1,15 * *'  # 6 AM UTC on 1st and 15th of each month
  workflow_dispatch:  # Manual trigger

permissions:
  id-token: write   # Required for OIDC
  contents: write   # Required for PR creation
  pull-requests: write

jobs:
  mirror:
    runs-on: ubuntu-latest
    steps:
      # ... implementation per steps above
```

### Tools

- Use `crane` (from `google/go-containerregistry`) for digest comparison — lightweight, no Docker daemon needed for digest checks
- Use Docker for pull/tag/push operations
- Use `gh` CLI for PR creation

---

## Phase 3: Update devcontainer.json Files

Update the image reference in both `devcontainer.json` files from:
```json
"image": "mcr.microsoft.com/devcontainers/base:noble"
```

To:
```json
"image": "public.ecr.aws/<ALIAS>/devcontainer-base:noble"
```

Where `<ALIAS>` is the ECR Public registry alias assigned during repository creation.

**Files to update:**
- `.devcontainer/devcontainer.json` — this repo's own development environment
- `collections/default/devcontainer.json` — the distributable default catalog collection

Both files use the same base image and must be kept in sync by the mirror workflow.

---

## Security Best Practices

- **No long-lived credentials** — GitHub Actions authenticates via OIDC, no AWS access keys stored as secrets
- **Least privilege IAM** — role can only push to the specific ECR Public repository, nothing else
- **Trust policy scoping** — IAM role can only be assumed by this specific repository's main branch
- **Image integrity** — digest comparison ensures we only push when upstream actually changes
- **Traceability** — version tags link back to the upstream digest for audit
- **No manual intervention** — workflow is fully automated, creates PRs for review rather than auto-merging

---

## Placeholders (to be filled during setup)

| Placeholder | Description |
|---|---|
| `<ACCOUNT_ID>` | Caylent Solutions Platform prod AWS account ID |
| `<REPO_NAME>` | GitHub repository name (e.g., `devcontainer`) |
| `<ALIAS>` | ECR Public registry alias (assigned at creation or customized) |

---

## Work Unit 3 Acceptance Criteria

- [ ] ECR Public repository exists and is accessible globally
- [ ] IAM OIDC provider configured for GitHub Actions
- [ ] IAM role with least-privilege policy created and trust-scoped to this repo
- [ ] GitHub Actions workflow runs on schedule (semi-monthly: 1st and 15th)
- [ ] Workflow correctly detects upstream image changes via digest comparison
- [ ] Workflow pushes new images to ECR Public only when upstream has changed
- [ ] Workflow creates a PR to update both `devcontainer.json` files when a new image is mirrored
- [ ] Both `.devcontainer/devcontainer.json` and `collections/default/devcontainer.json` reference the ECR Public image
- [ ] Developers in South America observe improved pull times
- [ ] No long-lived AWS credentials stored in GitHub
